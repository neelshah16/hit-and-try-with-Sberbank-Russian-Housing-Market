{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Prepration\n",
    "1. Import of Data\n",
    "2. Appending Test (Using mean as output class) and Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_extra = pd.read_csv('macro.csv')\n",
    "\n",
    "# End Submission output \n",
    "output = pd.DataFrame()\n",
    "output['id'] = df_test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30471\n",
      "7662\n",
      "38133\n"
     ]
    }
   ],
   "source": [
    "print(np.size(df_train, 0))\n",
    "print(np.size(df_test, 0))\n",
    "\n",
    "df_test['price_doc'] = df_train['price_doc'].mean()\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "print(np.size(df,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Data\n",
    "1. Prepare Initial Report.(Dtype, sample_data, max, min, mean, std, median,Correlation with price_doc, top_corr, missing_values_count, missing_values_percentage, Drop it, Importance_level, Comment)\n",
    "2. Go though full data (all columns) and figure out 10% of most important feature (Using corelation and logic).\n",
    "3. Droping unnecessary tables.\n",
    "4. Molding data that is not showing proper corelation. (create, modify ...etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments: \n",
      "      Unnamed: 0                                            Comment\n",
      "85   build_year        Need to modify using year sold - year build\n",
      "98     full_all  need to modify it to population density with area\n",
      "126    material                       bin to classify proper group\n",
      "170       floor     make a feature of roof house or 1 floor hosuse\n",
      "181       state                       bin to classify proper group\n",
      "289    sub_area                                bin and pop density\n",
      "291   timestamp                       break it into Year and month \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Missing Value: \n",
      "                     Unnamed: 0    Dtype  missing_values_percentage\n",
      "4                 school_quota  float64                   0.219389\n",
      "5            build_count_panel  float64                   0.163795\n",
      "8             build_count_slag  float64                   0.163795\n",
      "11           build_count_frame  float64                   0.163795\n",
      "12             build_count_mix  float64                   0.163795\n",
      "13            build_count_wood  float64                   0.163795\n",
      "25             preschool_quota  float64                   0.219487\n",
      "38    railroad_station_walk_km  float64                   0.000820\n",
      "39   railroad_station_walk_min  float64                   0.000820\n",
      "59               metro_km_walk  float64                   0.000820\n",
      "85                  build_year  float64                   0.446490\n",
      "89            build_count_foam  float64                   0.163795\n",
      "102                   kitch_sq  float64                   0.314135\n",
      "103          build_count_block  float64                   0.163795\n",
      "126                   material  float64                   0.314135\n",
      "149                  max_floor  float64                   0.314135\n",
      "170                      floor  float64                   0.005481\n",
      "181                      state  float64                   0.444980\n",
      "182       build_count_monolith  float64                   0.163795\n",
      "213        hospital_beds_raion  float64                   0.473926\n",
      "232                    life_sq  float64                   0.209478\n",
      "236          build_count_brick  float64                   0.163795\n",
      "275                   num_room  float64                   0.314135 \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:82: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "def initial_report(df, output_class, path):\n",
    "    '''\n",
    "    Returns a csv file that is very usefull in making initial assumptions\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        df : Dataframe\n",
    "        Output Class : Class to predict (string)\n",
    "        path : Path for csv to Save\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        Nothing\n",
    "    \n",
    "    '''\n",
    "    df_row_count = np.size(df, axis=0)\n",
    "    columns = df.columns\n",
    "    df_na = df.dropna()\n",
    "    report_csv = pd.DataFrame({}, index=columns.values)\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Adding Dtypes Info\n",
    "    report_csv['Dtype'] = df.dtypes\n",
    "\n",
    "    # Adding 5 sample to read from\n",
    "    for i in range(1, 6):\n",
    "        report_csv.loc[:, 'sample_data_'+str(i)] = df_na.iloc[i]\n",
    "\n",
    "    # other imp data\n",
    "    report_csv['max'] = df_na.max()\n",
    "    report_csv['min'] = df_na.min()\n",
    "    report_csv['mean'] = df_na.mean()\n",
    "    report_csv['std'] = df_na.std()\n",
    "    report_csv['median'] = df_na.median()\n",
    "\n",
    "    for j in df.select_dtypes(exclude=['object']).columns:\n",
    "        top_corr = list(corr[j].sort_values(ascending=False).index)\n",
    "        for i in range(1, 6):\n",
    "            report_csv.loc[j, 'top_corr_'+str(i)] = top_corr[i]\n",
    "\n",
    "    # missing value\n",
    "    report_csv['missing_values_count'] = df.isnull().sum()\n",
    "    report_csv['missing_values_percentage'] = (report_csv['missing_values_count']/df_row_count)\n",
    "    \n",
    "    # Adding correlation with output class\n",
    "    report_csv['Correlation with '+output_class] = corr[output_class]\n",
    "\n",
    "    report_csv['Importance_level'] = \"Average\"\n",
    "    report_csv['Comment'] = \"N/a\"\n",
    "\n",
    "    report_csv.to_csv(path)\n",
    "\n",
    "    \n",
    "# initial_report(df_train, 'price_doc', 'final/initial_report.csv')\n",
    "\n",
    "def initial_report_output(path):\n",
    "    '''\n",
    "    Load the conclusion from the CSV file\n",
    "    \n",
    "    Importance_level: Drop -- drop_list\n",
    "                      High -- imp_list\n",
    "    \n",
    "    Comment -> Print Conclusions\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        path : Path for csv to Save\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        [drop_list, imp_list]\n",
    "    \n",
    "    '''\n",
    "    report_csv = pd.read_csv(path)\n",
    "    drop_feature_list = report_csv[report_csv.Importance_level == \"Drop\"].iloc[:,0]\n",
    "    imp_feature_list = report_csv[report_csv.Importance_level == \"High\"].iloc[:,0]\n",
    "    drop_list = list(drop_feature_list)\n",
    "    imp_list = list(imp_feature_list)\n",
    "    comments = report_csv[report_csv.Comment != \"N/a\"].iloc[:,[0,-1]]\n",
    "    print('Comments: \\n', comments, '\\n\\n\\n\\n')\n",
    "    \n",
    "    missing_table = report_csv[report_csv.Importance_level != \"Drop\"][report_csv.missing_values_percentage > 0].iloc[:,[0,1,18]]\n",
    "    print('Missing Value: \\n', missing_table, '\\n\\n\\n\\n')\n",
    "    \n",
    "    return [drop_list, imp_list]\n",
    "\n",
    "[drop_list, imp_list] = initial_report_output('final/initial_report.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new features:\n",
    "1. Breaking feature into possible sessions(date to month and week) or simplifications of existing feature or Bining values together .\n",
    "2. Combination of existing feature (+, -, *, /,) (May use weights)(Completely manual) \n",
    "3. Remove Duplicate and unrelated features using (col vs y plot)\n",
    "4. Polynomials of top feature (2nd, 3rd, 2nd root, 3rd root, exp) (Use algo to check corelations)\n",
    "\n",
    "*Example Strategy for breaking data*\n",
    "1. From name we can extract the position and status ex Dr. Ms. etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# timestamp breaking\n",
    "df['sold_year'], df['month'], _ = df['timestamp'].str.split('-').str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Age of city\n",
    "df['city_build'] = df[['build_count_1971-1995',\n",
    "'build_count_1921-1945',\n",
    "'build_count_after_1995',\n",
    "'build_count_1946-1970',\n",
    "'build_count_before_1920']].idxmax(axis=1).str.replace('er_','-').str.replace('re_','-').str.split('-').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Material of city\n",
    "df['city_build_material'] = df[['build_count_panel',\n",
    "'build_count_slag',\n",
    "'build_count_frame',\n",
    "'build_count_mix',\n",
    "'build_count_wood',\n",
    "'build_count_foam',\n",
    "'build_count_block',\n",
    "'build_count_monolith',\n",
    "'build_count_brick']].idxmax(axis=1).str.replace('build_count_','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# city population distribution\n",
    "df['pop_density'] = df.work_all/df.area_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df[\"material\"] = df[\"material\"]**3\n",
      "df[\"state\"] = df[\"state\"]**3\n",
      "df[\"area_m\"] = df[\"area_m\"]**3\n",
      "df[\"school_quota\"] = np.log1p(df[\"school_quota\"])\n",
      "df[\"water_treatment_km\"] = np.log1p(df[\"water_treatment_km\"])\n",
      "df[\"incineration_km\"] = np.expm1(df[\"incineration_km\"])\n",
      "df[\"railroad_station_avto_km\"] = df[\"railroad_station_avto_km\"]**3\n",
      "df[\"public_transport_station_min_walk\"] = df[\"public_transport_station_min_walk\"]**2\n",
      "df[\"mkad_km\"] = np.expm1(df[\"mkad_km\"])\n",
      "df[\"big_road1_km\"] = df[\"big_road1_km\"]**2\n",
      "df[\"big_road2_km\"] = df[\"big_road2_km\"]**3\n",
      "df[\"railroad_km\"] = df[\"railroad_km\"]**3\n",
      "df[\"bus_terminal_avto_km\"] = df[\"bus_terminal_avto_km\"]**3\n",
      "df[\"oil_chemistry_km\"] = np.expm1(df[\"oil_chemistry_km\"])\n",
      "df[\"nuclear_reactor_km\"] = df[\"nuclear_reactor_km\"]**3\n",
      "df[\"power_transmission_line_km\"] = df[\"power_transmission_line_km\"]**2\n",
      "df[\"market_shop_km\"] = df[\"market_shop_km\"]**3\n",
      "df[\"hospice_morgue_km\"] = df[\"hospice_morgue_km\"]**2\n",
      "df[\"detention_facility_km\"] = np.expm1(df[\"detention_facility_km\"])\n",
      "df[\"university_km\"] = df[\"university_km\"]**2\n",
      "df[\"museum_km\"] = df[\"museum_km\"]**3\n",
      "df[\"exhibition_km\"] = df[\"exhibition_km\"]**2\n",
      "df[\"catering_km\"] = np.log1p(df[\"catering_km\"])\n",
      "df[\"green_part_5000\"] = df[\"green_part_5000\"]**3\n",
      "df[\"pop_density\"] = df[\"pop_density\"]**(1/3)\n"
     ]
    }
   ],
   "source": [
    "def corr(X,Y):\n",
    "    df1 = pd.DataFrame()\n",
    "    df1['x'] = X\n",
    "    df1['y'] = Y\n",
    "    return df1.corr().iloc[1,0]\n",
    "\n",
    "def corr_poly(df, output_class, varible_name):\n",
    "    output = df[output_class]\n",
    "    for i, col in enumerate(df.select_dtypes(exclude=['object']).columns):\n",
    "        if df[col].min() > 0:\n",
    "            column_data = [0,0,0,0,0,0,0,0]\n",
    "            column_data[0] = df[col]\n",
    "            column_data[1] = column_data[0]**2\n",
    "            column_data[2] = column_data[0]**3\n",
    "            column_data[3] = column_data[0]**0.5\n",
    "            column_data[4] = column_data[0]**(1/3)\n",
    "            column_data[5] = np.expm1(column_data[0])\n",
    "            column_data[6] = np.log1p(column_data[0])\n",
    "            column_data[7] = 1/(1+column_data[0])\n",
    "\n",
    "            corr_column_data = [0,0,0,0,0,0,0,0]\n",
    "            for j in range(7):\n",
    "                corr_column_data[j] = abs(corr(column_data[j],output))\n",
    "\n",
    "            k = corr_column_data.index(max(corr_column_data))\n",
    "\n",
    "            if k == 1:\n",
    "                print(varible_name+'[\"'+col+'\"] = '+varible_name+'[\"'+col+'\"]**2')\n",
    "            elif k == 2:\n",
    "                print(varible_name+'[\"'+col+'\"] = '+varible_name+'[\"'+col+'\"]**3')\n",
    "            elif k == 3:\n",
    "                print(varible_name+'[\"'+col+'\"] = '+varible_name+'[\"'+col+'\"]**0.5')\n",
    "            elif k == 4:\n",
    "                print(varible_name+'[\"'+col+'\"] = '+varible_name+'[\"'+col+'\"]**(1/3)')\n",
    "            elif k == 5:\n",
    "                print(varible_name+'[\"'+col+'\"] = np.expm1('+varible_name+'[\"'+col+'\"])')\n",
    "            elif k == 6:\n",
    "                print(varible_name+'[\"'+col+'\"] = np.log1p('+varible_name+'[\"'+col+'\"])')\n",
    "            elif k == 7:\n",
    "                print(varible_name+'[\"'+col+'\"] = 1/(1+'+varible_name+'[\"'+col+'\"])')\n",
    "        \n",
    "            \n",
    "# corr_poly(df, 'price_doc', 'df')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"material\"] = df[\"material\"]**3\n",
    "df[\"state\"] = df[\"state\"]**3\n",
    "df[\"area_m\"] = df[\"area_m\"]**3\n",
    "df[\"school_quota\"] = np.log1p(df[\"school_quota\"])\n",
    "df[\"water_treatment_km\"] = np.log1p(df[\"water_treatment_km\"])\n",
    "df[\"incineration_km\"] = np.expm1(df[\"incineration_km\"])\n",
    "df[\"railroad_station_avto_km\"] = df[\"railroad_station_avto_km\"]**3\n",
    "df[\"public_transport_station_min_walk\"] = df[\"public_transport_station_min_walk\"]**2\n",
    "df[\"mkad_km\"] = np.expm1(df[\"mkad_km\"])\n",
    "df[\"big_road1_km\"] = df[\"big_road1_km\"]**2\n",
    "df[\"big_road2_km\"] = df[\"big_road2_km\"]**3\n",
    "df[\"railroad_km\"] = df[\"railroad_km\"]**3\n",
    "df[\"bus_terminal_avto_km\"] = df[\"bus_terminal_avto_km\"]**3\n",
    "df[\"oil_chemistry_km\"] = np.expm1(df[\"oil_chemistry_km\"])\n",
    "df[\"nuclear_reactor_km\"] = df[\"nuclear_reactor_km\"]**3\n",
    "df[\"power_transmission_line_km\"] = df[\"power_transmission_line_km\"]**2\n",
    "df[\"market_shop_km\"] = df[\"market_shop_km\"]**3\n",
    "df[\"hospice_morgue_km\"] = df[\"hospice_morgue_km\"]**2\n",
    "df[\"detention_facility_km\"] = np.expm1(df[\"detention_facility_km\"])\n",
    "df[\"university_km\"] = df[\"university_km\"]**2\n",
    "df[\"museum_km\"] = df[\"museum_km\"]**3\n",
    "df[\"exhibition_km\"] = df[\"exhibition_km\"]**2\n",
    "df[\"catering_km\"] = np.log1p(df[\"catering_km\"])\n",
    "df[\"green_part_5000\"] = df[\"green_part_5000\"]**3\n",
    "df[\"pop_density\"] = df[\"pop_density\"]**(1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Droping Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop UnNecessary Columns\n",
    "drop_column = ['0_13_female','0_13_male','0_17_all','0_17_female','0_17_male','0_6_all','0_6_female','0_6_male','16_29_all','16_29_female','16_29_male','7_14_all','7_14_female','7_14_male','big_church_count_1000','big_church_count_1500','big_church_count_2000','big_church_count_3000','big_church_count_500','big_church_count_5000','big_market_km','build_count_1921-1945','build_count_1946-1970','build_count_1971-1995','build_count_after_1995','build_count_before_1920','cafe_avg_price_1000','cafe_avg_price_1500','cafe_avg_price_2000','cafe_avg_price_3000','cafe_avg_price_500','cafe_avg_price_5000','cafe_count_1000_na_price','cafe_count_1000_price_1000','cafe_count_1000_price_1500','cafe_count_1000_price_2500','cafe_count_1000_price_4000','cafe_count_1000_price_500','cafe_count_1000_price_high','cafe_count_1500','cafe_count_1500_na_price','cafe_count_1500_price_1000','cafe_count_1500_price_1500','cafe_count_1500_price_2500','cafe_count_1500_price_4000','cafe_count_1500_price_500','cafe_count_1500_price_high','cafe_count_2000_na_price','cafe_count_2000_price_1000','cafe_count_2000_price_1500','cafe_count_2000_price_2500','cafe_count_2000_price_4000','cafe_count_2000_price_500','cafe_count_2000_price_high','cafe_count_3000_na_price','cafe_count_3000_price_1000','cafe_count_3000_price_1500','cafe_count_3000_price_2500','cafe_count_3000_price_4000','cafe_count_3000_price_500','cafe_count_3000_price_high','cafe_count_500_na_price','cafe_count_500_price_1000','cafe_count_500_price_1500','cafe_count_500_price_2500','cafe_count_500_price_4000','cafe_count_500_price_500','cafe_count_500_price_high','cafe_count_5000_na_price','cafe_count_5000_price_1000','cafe_count_5000_price_1500','cafe_count_5000_price_2500','cafe_count_5000_price_4000','cafe_count_5000_price_500','cafe_count_5000_price_high','cafe_sum_1000_max_price_avg','cafe_sum_1000_min_price_avg','cafe_sum_1500_max_price_avg','cafe_sum_1500_min_price_avg','cafe_sum_2000_max_price_avg','cafe_sum_2000_min_price_avg','cafe_sum_3000_max_price_avg','cafe_sum_3000_min_price_avg','cafe_sum_500_max_price_avg','cafe_sum_500_min_price_avg','cafe_sum_5000_max_price_avg','cafe_sum_5000_min_price_avg','church_count_1000','church_count_1500','church_count_2000','church_count_3000','church_count_500','ekder_female','ekder_male','female_f','green_part_1000','green_part_1500','green_part_2000','green_part_3000','green_part_500','green_zone_km','green_zone_part','id','ID_big_road1','ID_big_road2','ID_bus_terminal','ID_metro','ID_railroad_station_avto','ID_railroad_station_walk','ID_railroad_terminal','leisure_count_1000','leisure_count_1500','leisure_count_2000','leisure_count_3000','leisure_count_500','male_f','market_count_1000','market_count_1500','market_count_2000','market_count_3000','market_count_500','mosque_count_1000','mosque_count_1500','mosque_count_2000','mosque_count_3000','mosque_count_500','office_count_1000','office_count_1500','office_count_2000','office_count_3000','office_count_500','office_count_5000','office_raion','office_sqm_1000','office_sqm_1500','office_sqm_2000','office_sqm_3000','office_sqm_500','prom_part_1000','prom_part_1500','prom_part_2000','prom_part_3000','prom_part_500','prom_part_5000','raion_build_count_with_builddate_info','raion_build_count_with_material_info','sport_count_1000','sport_count_1500','sport_count_2000','sport_count_3000','sport_objects_raion','trc_count_1000','trc_count_1500','trc_count_2000','trc_count_500','trc_count_5000','trc_sqm_1000','trc_sqm_1500','trc_sqm_2000','trc_sqm_3000','trc_sqm_500','trc_sqm_5000','work_female','work_male','young_female','young_male']\n",
    "\n",
    "def find_similar_feature(df, output_class_name, threshold=0.75):\n",
    "    df = pd.DataFrame(df)\n",
    "    corr_with_out = df[output_class_name]\n",
    "\n",
    "    df.drop(output_class_name, 1, inplace=True)\n",
    "    df.drop(output_class_name, 0, inplace=True)\n",
    "\n",
    "    row_count = np.size(df, axis=0)\n",
    "    col_count = np.size(df, axis=1)\n",
    "    \n",
    "    if row_count != col_count:\n",
    "        raise ValueError('Data is Unsymmetric')\n",
    "        \n",
    "    drop_list = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0, row_count):\n",
    "        for j in range(0, i):\n",
    "            if df.iloc[i,j] >= threshold:\n",
    "                if(abs(corr_with_out[i]) >= abs(corr_with_out[j])):\n",
    "                    drop_list = drop_list.append([df.columns[i]])\n",
    "                else:\n",
    "                    drop_list = drop_list.append([df.columns[j]])\n",
    "#                 print(df.columns[i], df.index[j],\"are simmilar\", df.iloc[i,j], \"with \", corr_with_out[i])\n",
    "                \n",
    "    return drop_list.drop_duplicates()[0]\n",
    "\n",
    "corr = df.corr()\n",
    "drop_list = find_similar_feature(corr, \"price_doc\", threshold=0.90)\n",
    "drop_column.extend(drop_list)\n",
    "df.drop(drop_column, 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important data-modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_missing_data_with_suitable(df):\n",
    "    # missing value fill with mode in case of Categorical feature\n",
    "    for i in df.select_dtypes(include=['object']).columns:\n",
    "        try:\n",
    "            df[i] = df[i].fillna(df[i].value_counts()[0])\n",
    "        except:\n",
    "            print(i)\n",
    "    \n",
    "    # missing value fill with mean in case of Numerical column\n",
    "    for i in df.select_dtypes(exclude=['object']).columns:\n",
    "        df[i] = df[i].fillna(df[i].mean())\n",
    "    \n",
    "    return df\n",
    "\n",
    "df.replace([np.inf, -np.inf], np.nan)\n",
    "df = fill_missing_data_with_suitable(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "from scipy.stats import skew\n",
    "\n",
    "skewed = df.select_dtypes(exclude=['object']).apply(lambda x: skew(x.astype(float)))\n",
    "skewed = skewed[abs(skewed) > 0.75].index\n",
    "df[skewed] = np.log1p(df[skewed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df.iloc[0:30471, :]\n",
    "df_test = df.iloc[30471:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "outlier_id = [3527, 13546, 23584, 20722, 21734, 21852, 25940, 10089, 11621,\n",
    "       26713, 10368, 13117, 21415, 28734, 11701, 12742, 14426, 14685,\n",
    "       14729, 14895, 15868, 15902, 16790, 17010, 17059, 17396, 17404,\n",
    "       17761, 18074, 18356, 18959, 20324, 20863, 21565, 21663, 21938,\n",
    "       22487, 23397, 23483, 23656, 24820, 25769, 25826, 25876, 26923,\n",
    "       27371, 28074, 29136,  7977, 22694, 16308, 22778,   723,   948,\n",
    "        1402,  2934,  6191,  8895, 10901, 11278, 12282, 17296, 18933,\n",
    "       18990, 22664, 23602, 26487,  8073,  9502, 29656,  8933,  1465,\n",
    "       10587, 16486, 28326,  7457,  2118]\n",
    "\n",
    "df_train.drop(outlier_id, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30394\n",
      "7662\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df_train.drop([\"price_doc\"], 1))\n",
    "X = preprocessing.scale(X)\n",
    "y = np.array(df_train[\"price_doc\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "print(np.size(df_train, 0))\n",
    "print(np.size(df_test, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error:  0.443215597196\n",
      "Testing Error:  0.472703522299\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "regr = xgb.XGBRegressor(booster=\"gbtree\", learning_rate=0.1)\n",
    "#                  colsample_bytree=0.2,\n",
    "#                  gamma=0.0,\n",
    "#                  learning_rate=0.05,\n",
    "#                  max_depth=6,\n",
    "#                  min_child_weight=1.5,\n",
    "#                  n_estimators=7200,                                                                  \n",
    "#                  reg_alpha=0.9,\n",
    "#                  reg_lambda=0.6,\n",
    "#                  subsample=0.2,\n",
    "#                  seed=42,\n",
    "#                  silent=1\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "y_train_ = regr.predict(X_train)\n",
    "print(\"Training Error: \",rmse(y_train,y_train_))\n",
    "\n",
    "y_test_ = regr.predict(X_test)\n",
    "print(\"Testing Error: \",rmse(y_test,y_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sub = np.array(df_test.drop([\"price_doc\"], 1))\n",
    "X_sub = preprocessing.scale(X_sub)\n",
    "y_sub = regr.predict(X_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output['price_doc'] = np.expm1(y_sub)\n",
    "output.to_csv(\"submission_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
